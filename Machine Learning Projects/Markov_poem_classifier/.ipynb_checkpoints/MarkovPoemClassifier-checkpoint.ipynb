{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67056eaa-fc5a-40e9-b899-a42e8020c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "# !wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0050fab9-104f-459f-88c9-d2bdbf625039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa17c388-d28b-450d-989c-b85c89bd6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sabyrkabylbek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sabyrkabylbek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sabyrkabylbek/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sabyrkabylbek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b6405d-22f7-43c9-9d01-c1c54b4229c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edgar_allan_poe.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_edg = [line.strip() for line in f]\n",
    "with open(\"robert_frost.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_rob = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2cc5c-2678-4eee-bba6-3ecccd40a8aa",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d2b176-41c0-4bc3-9fd3-6c94d4a94ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blank or white-space lines\n",
    "lines_edg = [i for i in lines_edg if i.strip() != '']\n",
    "lines_rob = [i for i in lines_rob if i.strip() != '']\n",
    "\n",
    "# make all lower case\n",
    "lines_edg = [i.lower() for i in lines_edg]\n",
    "lines_rob = [i.lower() for i in lines_rob]\n",
    "\n",
    "# unicode normalization\n",
    "lines_edg = [unicodedata.normalize(\"NFKC\", i) for i in lines_edg]\n",
    "lines_rob = [unicodedata.normalize(\"NFKC\", i) for i in lines_rob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ff9f21-8cec-4610-9896-bee358eb8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(line):\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', line, re.UNICODE)\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) if token.isalpha() else token for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if not (token.lower() in stop_words and token.isalpha())]\n",
    "    \n",
    "\n",
    "lines_edg = [tokenize(i) for i in lines_edg]\n",
    "lines_rob = [tokenize(i) for i in lines_rob]\n",
    "\n",
    "lines_edg = [lemmatize_tokens(i) for i in lines_edg]\n",
    "lines_rob = [lemmatize_tokens(i) for i in lines_rob]\n",
    "\n",
    "lines_edg = [remove_stopwords(i) for i in lines_edg]\n",
    "lines_rob = [remove_stopwords(i) for i in lines_rob]\n",
    "\n",
    "lines_edg = [i for i in lines_edg if len(i) != 0]\n",
    "lines_rob = [i for i in lines_rob if len(i) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddef3427-e419-4ee0-a4b7-39922404c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels\n",
    "lines_edg_lb = [[i, 0] for i in lines_edg] # for edgar - 0\n",
    "lines_rob_lb = [[i, 1] for i in lines_rob] # for rob - 1\n",
    "all_lb_lines = lines_edg_lb + lines_rob_lb\n",
    "\n",
    "# separate all lines and labels\n",
    "all_lines = [i[0] for i in all_lb_lines]\n",
    "all_labels = [i[1] for i in all_lb_lines]\n",
    "\n",
    "# split training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    all_lines, all_labels, \n",
    "    test_size = 0.2, random_state = 42, stratify=all_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0177d6a-2e9f-4772-ba4c-a22d0e0bd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to index\n",
    "def word_idx_map(line_token_list: list()) -> dict:\n",
    "    word_idx = {'<unk>': 0}\n",
    "    idx = 1\n",
    "    for line_token in line_token_list:\n",
    "        for token in line_token:\n",
    "            if token not in word_idx:\n",
    "                word_idx[token] = idx\n",
    "                idx += 1\n",
    "    return word_idx\n",
    "    \n",
    "# Convert words list to its' index list\n",
    "def word_idx_convert(line_token_list: list(), word2idx: dict()) -> list:\n",
    "    text_int = []\n",
    "    for line_token in line_token_list:\n",
    "        line_int = [word2idx.get(token, 0) for token in line_token]\n",
    "        text_int.append(line_int)\n",
    "    return text_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f03cf6b7-f990-450d-bc86-f268ddd4cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = word_idx_map(X_train)\n",
    "X_train_int = word_idx_convert(X_train, word2idx)\n",
    "X_test_int = word_idx_convert(X_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4808cfc-6837-48a5-93ce-aaf938734c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the matrices A and P for each class\n",
    "V = len(word2idx)\n",
    "A0 = np.ones((V, V))\n",
    "A1 = np.ones((V, V))\n",
    "\n",
    "pi0 = np.ones(V)\n",
    "pi1 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe90fc67-2fbd-4446-89da-79e5707f7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_transitions(text_int, A, pi):\n",
    "    for line_int in text_int:\n",
    "        last_idx = None\n",
    "        for idx in line_int:\n",
    "            if last_idx is None:\n",
    "                pi[idx] += 1\n",
    "            else:\n",
    "                A[last_idx, idx] += 1\n",
    "            last_idx = idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f12fd15-c49f-475d-ad95-588110009058",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_transitions([t for t, y in zip(X_train_int, Y_train) if y == 0], A0, pi0)\n",
    "count_transitions([t for t, y in zip(X_train_int, Y_train) if y == 1], A1, pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afca87f2-2748-411d-9860-e6049393ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 /= A0.sum(axis = 1, keepdims = True)\n",
    "A1 /= A1.sum(axis = 1, keepdims = True)\n",
    "\n",
    "pi0 /= pi0.sum()\n",
    "pi1 /= pi1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "162e5bf3-1f66-478b-853d-39d055a2dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logA0 = np.log(A0)\n",
    "logA1 = np.log(A1)\n",
    "logpi0 = np.log(pi0)\n",
    "logpi1 = np.log(pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf6cc853-cc1e-495a-a3ec-b9ec662e9296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3335270191748983, 0.6664729808251016)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute priors\n",
    "count0 = sum(y == 0 for y in Y_train)\n",
    "count1 = sum(y == 1 for y in Y_train)\n",
    "total = len(Y_train)\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)\n",
    "p0, p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f3642ed-03a2-4cde-adb2-5b9bf64273f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "class Classifier:\n",
    "  def __init__(self, logAs, logpis, logpriors):\n",
    "    self.logAs = logAs\n",
    "    self.logpis = logpis\n",
    "    self.logpriors = logpriors\n",
    "    self.K = len(logpriors) # number of classes\n",
    "\n",
    "  def _compute_log_likelihood(self, input_, class_):\n",
    "    logA = self.logAs[class_]\n",
    "    logpi = self.logpis[class_]\n",
    "\n",
    "    last_idx = None\n",
    "    logprob = 0\n",
    "    for idx in input_:\n",
    "      if last_idx is None:\n",
    "        # it's the first token\n",
    "        logprob += logpi[idx]\n",
    "      else:\n",
    "        logprob += logA[last_idx, idx]\n",
    "      \n",
    "      # update last_idx\n",
    "      last_idx = idx\n",
    "    \n",
    "    return logprob\n",
    "  \n",
    "  def predict(self, inputs):\n",
    "    predictions = np.zeros(len(inputs))\n",
    "    for i, input_ in enumerate(inputs):\n",
    "      posteriors = [self._compute_log_likelihood(input_, c) + self.logpriors[c] \\\n",
    "             for c in range(self.K)]\n",
    "      pred = np.argmax(posteriors)\n",
    "      predictions[i] = pred\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eef1097-702f-4f0a-9176-448cc6c20d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array must be in order since classes are assumed to index these lists\n",
    "clf = Classifier([logA0, logA1], [logpi0, logpi1], [logp0, logp1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c212c931-20d4-496f-bc6d-ba8e06a1690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.9953515398024404\n"
     ]
    }
   ],
   "source": [
    "Ptrain = clf.predict(X_train_int)\n",
    "print(f\"Train acc: {np.mean(Ptrain == Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df071ff9-99f2-429b-bb3f-f05c48fb8d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.8190255220417634\n"
     ]
    }
   ],
   "source": [
    "Ptest = clf.predict(X_test_int)\n",
    "print(f\"Test acc: {np.mean(Ptest == Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d72c33a9-2f7d-4571-a73e-8819b08cef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1932f36-6f89-4561-a99d-cbe9ba18372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 566,    8],\n",
       "       [   0, 1147]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_train, Ptrain)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb6d86bd-fda2-4051-9732-4ef90279a46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 77,  67],\n",
       "       [ 11, 276]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test = confusion_matrix(Y_test, Ptest)\n",
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fcbd268-6d1e-4f01-b2db-e0fa84f4e6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996524761077324"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_train, Ptrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed44e561-69c1-4272-ae43-b487b0dbdce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8761904761904762"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_test, Ptest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2044e6-061d-4dee-beb6-133462d6dbac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
